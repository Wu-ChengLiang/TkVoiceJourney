#!/usr/bin/env python3
"""
ç®€å•å¯¹è¯è„šæœ¬ - ä¿®å¤ç‰ˆ 
ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹è·¯å¾„ï¼Œä¸“ä¸ºRTX 4060 8GBæ˜¾å­˜ä¼˜åŒ–
"""

import os
import sys
import torch
import gc
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

def clear_memory():
    """æ¸…ç†æ˜¾å­˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

def find_model_path():
    """æŸ¥æ‰¾æ¨¡å‹è·¯å¾„"""
    current_dir = Path.cwd()
    
    # å¯èƒ½çš„æ¨¡å‹è·¯å¾„
    possible_paths = [
        current_dir / "my_models", 
        current_dir / "my_models_backup",
        
        current_dir.parent / "my_models",
        current_dir / "qwen3_models"
    ]
    
    for path in possible_paths:
        if path.exists() and (path / "config.json").exists():
            return path
    
    return None

def setup_model():
    """è®¾ç½®æ¨¡å‹ - ä½¿ç”¨æœ€æ¿€è¿›çš„æ˜¾å­˜ä¼˜åŒ–"""
    
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    clear_memory()
    
    # æŸ¥æ‰¾æ¨¡å‹è·¯å¾„
    base_model_path = find_model_path()
    if base_model_path is None:
        print("âŒ æœªæ‰¾åˆ°åŸºç¡€æ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶")
        return None, None
    
    current_dir = Path.cwd()
    # lora_path = current_dir / "qwen3_output" / "v7-20250529-224742" / "checkpoint-33"
    lora_path = current_dir / "qwen3_output" / "v10-20250530-074122" / "checkpoint-33"
    

    print(f"åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"LoRAæ¨¡å‹: {lora_path}")
    
    # æœ€æ¿€è¿›çš„4bité‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    try:
        # åŠ è½½tokenizer
        print("ğŸ“ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            str(base_model_path),
            trust_remote_code=True,
            padding_side="left"
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            str(base_model_path),
            quantization_config=bnb_config,
            device_map="auto",
            torch_dtype=torch.float16,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        
        # åŠ è½½LoRAæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if lora_path.exists():
            print("ğŸ”§ åŠ è½½LoRAé€‚é…å™¨...")
            model = PeftModel.from_pretrained(model, str(lora_path))
            print("âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸  LoRAæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
        
        # è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
        model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        clear_memory()
        return None, None

def format_conversation_prompt(history, user_input):
    """æ ¼å¼åŒ–å¯¹è¯prompt"""
    
    # ä½¿ç”¨ç¾å›¢å®¢æœçš„æ ¼å¼
    prompt_parts = []
    
    # æ·»åŠ ç³»ç»Ÿæç¤º
    prompt_parts.append("""
    1. ä½ æ˜¯ååŒ»å ‚çš„ç¾å›¢ä¸“ä¸šå®¢æœåŠ©ç†ï¼ŒååŒ»å ‚ç§‰æ‰¿â€˜ä¼ æ‰¿ä¸­åŒ»æ–‡åŒ–ï¼ŒæœåŠ¡ç™¾å§“å¥åº·â€™ç†å¿µï¼Œä¸“æ³¨æä¾›â€˜åŒ»å…»ç»“åˆâ€™çš„ä¸­åŒ»ç†ç–—æœåŠ¡ã€‚ååŒ»å ‚è®©ä¼˜è´¨ä¸­åŒ»èµ°è¿›ç¤¾åŒºï¼Œä¸ºé¡¾å®¢è§£å†³ç–¼ç—›ã€äºšå¥åº·åŠè„è…‘è°ƒç†é—®é¢˜ã€‚
    2. è¯­æ°”ï¼šè¦ä¸“ä¸šã€ç®€çŸ­ã€æ¸©æŸ” ä¸è¦ç”¨!æ„Ÿå¹å· 
    3. ä¸è¦æ‰¿è¯ºä½ çŸ¥è¯†èŒƒç•´ä¹‹å¤–çš„äº‹æƒ…ï¼Œå¦‚æœè¶…å‡ºäº†ä½ çš„å›ç­”èŒƒå›´ï¼Œè¯·å›å¤ï¼šç¨ç­‰ä¸€ä¸‹ï¼Œæˆ‘å’Œè€å¸ˆç¡®è®¤ä¸€ä¸‹ï¼Œç¨åå›å¤æ‚¨
    4. æ‰€æœ‰çš„å…·ä½“ä¿¡æ¯éƒ½ç”¨xxæ¥è¡¨ç¤º å¦‚ç”µè¯\å§“å\æ—¶é—´\åœ°ç‚¹
    """)
    
    # æ·»åŠ å¯¹è¯å†å²
    for i in range(0, len(history), 2):
        if i + 1 < len(history):
            user_msg = history[i].replace("ç”¨æˆ·: ", "")
            assistant_msg = history[i + 1].replace("åŠ©æ‰‹: ", "")
            prompt_parts.append(f"user: {user_msg}")
            prompt_parts.append(f"assistant: {assistant_msg}")
    
    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
    prompt_parts.append(f"user: {user_input}")
    prompt_parts.append("assistant: ")
    
    return "\n".join(prompt_parts)

def chat_with_model(model, tokenizer):
    """ä¸æ¨¡å‹å¯¹è¯"""
    
    if model is None or tokenizer is None:
        print("âŒ æ¨¡å‹æœªåŠ è½½")
        return
    
    print("\nğŸ¯ ç¾å›¢å®¢æœåŠ©æ‰‹å·²å°±ç»ªï¼")
    print("è¾“å…¥ 'quit'/'exit'/'é€€å‡º' ç»“æŸå¯¹è¯")
    print("=" * 50)
    
    conversation_history = []
    
    # æµ‹è¯•é—®é¢˜å»ºè®®
    test_questions = [
        "åœ¨å—ï¼Ÿæƒ³é¢„çº¦ä¸‹åˆXXç‚¹",
        "æ–¹ä¾¿åœè½¦å—ï¼Ÿ",
        "XXè€å¸ˆåœ¨å—ï¼Ÿé¢„çº¦ä»–XXç‚¹",
        "ä½ ä»¬è¥ä¸šåˆ°å‡ ç‚¹?"
        "1ä½ï¼Œæœ‰å¥³è€å¸ˆå—ï¼Ÿ",
    ]

    print("ğŸ’¡ å»ºè®®æµ‹è¯•é—®é¢˜:")
    for i, q in enumerate(test_questions, 1):
        print(f"  {i}. {q}")
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("\nğŸª user: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ç¾å›¢å®¢æœæœåŠ¡ï¼Œå†è§!")
            break
        
        if not user_input:
            continue
        
        try:
            # æ ¼å¼åŒ–prompt
            prompt = format_conversation_prompt(conversation_history, user_input)
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                max_length=1024,  # é€‚ä¸­çš„è¾“å…¥é•¿åº¦
                truncation=True,
                padding=True
            ).to(model.device)
            
            # ç”Ÿæˆå›å¤
            print("ğŸ¤– å®¢æœæ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢...")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    max_new_tokens=200,  # é€‚ä¸­çš„è¾“å‡ºé•¿åº¦ ï¼›è¦å…³é—­æ€è€ƒæ¨¡å¼
                    do_sample=True,
                    temperature=0.2,
                    top_p=0.9,
                    top_k=40,
                    repetition_penalty=1.05,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # è§£ç å›å¤
            generated_text = tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            

            # æ¸…ç†å›å¤æ–‡æœ¬
            response = generated_text.split("assistant:")[0].strip()
            
            #  # æ·»åŠ åå¤„ç†ï¼šå¦‚æœå›å¤ä¸­åŒ…å«æ¢è¡Œç¬¦ï¼Œåˆ™åªå–ç¬¬ä¸€è¡Œå†…å®¹ï¼›æˆ–æ˜¯ç¦æ­¢userå†…å®¹ç”Ÿæˆ
            # if '\n' in response:
            #     response = response.split('\n')[0].strip()

            if not response:
                response = "ç¨ç­‰ï¼Œæˆ‘éœ€è¦æŸ¥è¯¢ä¸€ä¸‹"

            #åœ¨è¿™é‡Œå¯ä»¥åˆ°ç”¨å·¥å…·

            # è¿™é‡Œå¯ä»¥ç”¨çŸ­ä¿¡é€šçŸ¥äººå‘˜
            
            print(f"ğŸ‘©â€ğŸ’¼ å®¢æœ: {response}")
            
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            conversation_history.append(f"user: {user_input}")
            conversation_history.append(f"assistant: {response}")
            
            # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†é•¿åº¦
            if len(conversation_history) > 8:
                conversation_history = conversation_history[-8:]
            
            # æ¸…ç†æ˜¾å­˜
            clear_memory()
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
            print("ğŸ”„ æ­£åœ¨æ¸…ç†æ˜¾å­˜ï¼Œè¯·é‡è¯•...")
            clear_memory()
            continue

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” ç¾å›¢å®¢æœåŠ©æ‰‹ v7")
    print("åŸºäº Qwen3-8B + LoRA å¾®è°ƒæ¨¡å‹")
    print("ä¸“ä¸ºRTX 4060 8GBæ˜¾å­˜ä¼˜åŒ–")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAï¼Œè¯·æ£€æŸ¥GPUé©±åŠ¨")
        return
    
    gpu_name = torch.cuda.get_device_name(0)
    print(f"ğŸ”¥ ä½¿ç”¨GPU: {gpu_name}")
    
    # æ˜¾å­˜æ£€æŸ¥
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    print(f"ğŸ’¾ æ€»æ˜¾å­˜: {total_memory:.1f}GB")
    
    if total_memory < 7:
        print("âš ï¸  è­¦å‘Š: æ˜¾å­˜ä¸è¶³8GBï¼Œå¯èƒ½å½±å“æ€§èƒ½")
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = setup_model()
    
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # å¼€å§‹å¯¹è¯
    chat_with_model(model, tokenizer)
    
    # æ¸…ç†èµ„æº
    clear_memory()
    print("ğŸ§¹ èµ„æºå·²æ¸…ç†")

if __name__ == "__main__":
    main() 