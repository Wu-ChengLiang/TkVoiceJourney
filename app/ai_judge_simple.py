#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆAIåˆ¤æ–­å™¨
ä¸“æ³¨äºå¼¹å¹•ä»·å€¼åˆ¤æ–­ï¼Œå›å¤ç”Ÿæˆäº¤ç»™ai_reply.pyå¤„ç†
"""

import asyncio
import hashlib
import logging
import re
import time
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, AsyncIterator
from dataclasses import dataclass
from enum import Enum

from config import AI_JUDGE_CONFIG, HIGH_VALUE_KEYWORDS
from ai_reply import create_reply_generator

logger = logging.getLogger(__name__)


class ProcessAction(Enum):
    """å¤„ç†åŠ¨ä½œæšä¸¾"""
    IGNORE = "ignore"
    PROCESS = "process"
    DEFER = "defer"
    BATCH = "batch"


@dataclass
class ProcessResult:
    """å¤„ç†ç»“æœ"""
    action: ProcessAction
    confidence: float = 0.0
    reason: str = ""
    delay: float = 0.0


@dataclass
class AIJudgeResult:
    """AIåˆ¤æ–­ç»“æœ"""
    has_value: bool
    content: str
    reason: str
    category: str
    confidence: float = 0.0
    template_id: Optional[str] = None


class LRUCache:
    """ç®€å•çš„LRUç¼“å­˜å®ç°"""
    
    def __init__(self, maxsize: int = 1000):
        self.maxsize = maxsize
        self.cache = {}
        self.access_order = deque()
    
    def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return None
    
    def put(self, key: str, value: Any):
        if key in self.cache:
            self.access_order.remove(key)
        elif len(self.cache) >= self.maxsize:
            oldest = self.access_order.popleft()
            del self.cache[oldest]
        
        self.cache[key] = value
        self.access_order.append(key)
    
    def __contains__(self, key: str) -> bool:
        return key in self.cache


class TokenBucket:
    """ä»¤ç‰Œæ¡¶é™æµå™¨"""
    
    def __init__(self, capacity: int = 100, refill_rate: float = 10.0):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate
        self.last_refill = time.time()
    
    def consume(self, tokens: int = 1) -> bool:
        """æ¶ˆè´¹ä»¤ç‰Œ"""
        self._refill()
        if self.tokens >= tokens:
            self.tokens -= tokens
            return True
        return False
    
    def _refill(self):
        """è¡¥å……ä»¤ç‰Œ"""
        now = time.time()
        tokens_to_add = (now - self.last_refill) * self.refill_rate
        self.tokens = min(self.capacity, self.tokens + tokens_to_add)
        self.last_refill = now


class KeywordFilter:
    """å…³é”®è¯è¿‡æ»¤å™¨"""
    
    def __init__(self):
        # ä»é…ç½®æ–‡ä»¶åŠ è½½å…³é”®è¯
        self.high_value_keywords = {}
        for category, keywords in HIGH_VALUE_KEYWORDS.items():
            for keyword in keywords:
                self.high_value_keywords[keyword] = 0.9
        
        # è´Ÿé¢å…³é”®è¯ï¼ˆé™ä½ä»·å€¼ï¼‰
        self.negative_keywords = {
            'å“ˆå“ˆ': -0.5, 'å‘µå‘µ': -0.5, 'å˜¿å˜¿': -0.5,
            '666': -0.5, '999': -0.5, 'ç‰›æ‰¹': -0.3,
            'å‰å®³': -0.3, 'ç‰›': -0.3, 'æ£’': -0.3,
            'åˆ·å±': -1.0, 'å¹¿å‘Š': -1.0, 'åŠ ç¾¤': -1.0,
            'æ¨å¹¿': -1.0, 'ä»£ç†': -1.0, 'æ‹›è˜': -1.0
        }
        
        # åƒåœ¾å†…å®¹æ¨¡å¼
        self.spam_patterns = [
            r'[0-9]{6,}',  # é•¿æ•°å­—ä¸²
            r'[a-zA-Z]{10,}',  # é•¿å­—æ¯ä¸²
            r'(.)\1{5,}',  # é‡å¤å­—ç¬¦
            r'[ï¼!]{3,}',  # å¤šä¸ªæ„Ÿå¹å·
            r'[ã€‚.]{3,}',  # å¤šä¸ªå¥å·
            r'http[s]?://',  # ç½‘å€
            r'QQ[:ï¼š]\s*\d+',  # QQå·
            r'å¾®ä¿¡[:ï¼š]',  # å¾®ä¿¡å·
        ]
    
    def calculate_score(self, content: str) -> float:
        """è®¡ç®—å†…å®¹ä»·å€¼åˆ†æ•°"""
        if not content or len(content.strip()) < 2:
            return 0.0
        
        content = content.strip().lower()
        
        # æ£€æŸ¥åƒåœ¾å†…å®¹
        if self._is_spam(content):
            return 0.0
        
        # è®¡ç®—å…³é”®è¯å¾—åˆ†
        score = 0.0
        
        # é«˜ä»·å€¼å…³é”®è¯
        for keyword, weight in self.high_value_keywords.items():
            if keyword in content:
                score += weight
        
        # è´Ÿé¢å…³é”®è¯
        for keyword, weight in self.negative_keywords.items():
            if keyword in content:
                score += weight
        
        # é•¿åº¦è°ƒæ•´
        length_bonus = min(0.2, len(content) / 100)
        score += length_bonus
        
        return max(0.0, min(1.0, score))
    
    def _is_spam(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºåƒåœ¾å†…å®¹"""
        for pattern in self.spam_patterns:
            if re.search(pattern, content):
                return True
        return False
    
    def detect_category(self, content: str) -> str:
        """æ£€æµ‹å†…å®¹ç±»åˆ«"""
        content = content.lower()
        
        for category, keywords in HIGH_VALUE_KEYWORDS.items():
            for keyword in keywords:
                if keyword in content:
                    return category
        
        return "å…¶ä»–"


class LocalClassifier:
    """æœ¬åœ°è½»é‡åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.keyword_filter = KeywordFilter()
        
        # é—®å¥æ¨¡å¼
        self.question_patterns = [
            r'[ï¼Ÿ?]',  # é—®å·
            r'æ€ä¹ˆ[^0-9]*',  # æ€ä¹ˆå¼€å¤´
            r'ä»€ä¹ˆ[^0-9]*',  # ä»€ä¹ˆå¼€å¤´
            r'å“ªé‡Œ[^0-9]*',  # å“ªé‡Œå¼€å¤´
            r'å‡ ç‚¹[^0-9]*',  # å‡ ç‚¹å¼€å¤´
            r'å¤šå°‘[^0-9]*',  # å¤šå°‘å¼€å¤´
            r'å¯ä»¥.*å—',  # å¯ä»¥...å—
            r'èƒ½.*å—',  # èƒ½...å—
            r'æœ‰.*å—',  # æœ‰...å—
        ]
    
    async def classify(self, content: str) -> float:
        """æœ¬åœ°åˆ†ç±»ï¼Œè¿”å›ç½®ä¿¡åº¦"""
        if not content:
            return 0.0
        
        # åŸºç¡€å…³é”®è¯å¾—åˆ†
        keyword_score = self.keyword_filter.calculate_score(content)
        
        # é—®å¥æ¨¡å¼å¾—åˆ†
        question_score = self._calculate_question_score(content)
        
        # ç»¼åˆå¾—åˆ†
        final_score = keyword_score * 0.7 + question_score * 0.3
        
        return min(1.0, final_score)
    
    def _calculate_question_score(self, content: str) -> float:
        """è®¡ç®—é—®å¥å¾—åˆ†"""
        score = 0.0
        for pattern in self.question_patterns:
            if re.search(pattern, content):
                score += 0.3
        return min(1.0, score)


class SmartBarrageProcessor:
    """æ™ºèƒ½å¼¹å¹•é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.dedup_cache = LRUCache(maxsize=1000)
        self.keyword_filter = KeywordFilter()
        self.local_classifier = LocalClassifier()
        self.rate_limiter = TokenBucket(
            capacity=AI_JUDGE_CONFIG["rate_limit"]["capacity"],
            refill_rate=AI_JUDGE_CONFIG["rate_limit"]["refill_rate"]
        )
        self.processed_hashes = set()
        
        # å»é‡æ—¶é—´çª—å£
        self.dedup_window = 300  # 5åˆ†é’Ÿ
    
    async def process_barrage(self, barrage: Dict) -> ProcessResult:
        """å¤„ç†å•æ¡å¼¹å¹•"""
        # 1. åŸºç¡€è¿‡æ»¤
        if not self._basic_filter(barrage):
            return ProcessResult(ProcessAction.IGNORE, reason="basic_filter")
        
        # 2. å»é‡æ£€æµ‹
        if self._is_duplicate(barrage):
            return ProcessResult(ProcessAction.IGNORE, reason="duplicate")
        
        # 3. é«˜ä»·å€¼å…³é”®è¯å¼ºåˆ¶å¤„ç†
        content = barrage.get('content', '')
        high_value_keywords = [
            'æµ‹è¯•', 'å’¨è¯¢', 'é¢„çº¦', 'ä»·æ ¼', 'å¤šå°‘é’±', 'è¥ä¸šæ—¶é—´', 'åœ°å€', 'ç”µè¯', 'æ€ä¹ˆ',
            'é“¾æ¥', 'å¥—é¤', 'å…ƒ', 'åŒ…å«', 'è¤', 'ç´ ', 'ä¸»é£Ÿ', 'é”…åº•', 'è˜¸æ–™', 'é—¨åº—',
            'åŒäººé¤', 'ä¸‰äººé¤', 'å››äººé¤', 'å…¨å›½', 'å¯¼èˆª', 'ä»Šå¤©æ‹'
        ]
        if any(kw in content for kw in high_value_keywords):
            return ProcessResult(ProcessAction.PROCESS, confidence=1.0, reason="high_value_keyword_match")
        
        # 4. å…³é”®è¯é¢„ç­›é€‰
        keyword_score = self.keyword_filter.calculate_score(content)
        if keyword_score < AI_JUDGE_CONFIG["keyword_threshold"]:
            return ProcessResult(ProcessAction.IGNORE, reason="low_keyword_score")
        
        # 5. æœ¬åœ°è½»é‡åˆ†ç±»
        local_score = await self.local_classifier.classify(content)
        if local_score < AI_JUDGE_CONFIG["local_score_threshold"]:
            return ProcessResult(ProcessAction.IGNORE, reason="low_local_score")
        
        # 6. é¢‘ç‡æ§åˆ¶
        if not self.rate_limiter.consume(1):
            return ProcessResult(ProcessAction.DEFER, reason="rate_limit", delay=2.0)
        
        # 7. å†³å®šå¤„ç†ç­–ç•¥
        if local_score > 0.8:
            return ProcessResult(ProcessAction.PROCESS, confidence=local_score)
        else:
            return ProcessResult(ProcessAction.BATCH, confidence=local_score)
    
    def _basic_filter(self, barrage: Dict) -> bool:
        """åŸºç¡€è¿‡æ»¤"""
        # æ£€æŸ¥ç±»å‹
        if barrage.get('type') not in ['chat', 'emoji_chat']:
            return False
        
        # æ£€æŸ¥å†…å®¹
        content = barrage.get('content', '').strip()
        if not content or len(content) < 2 or len(content) > 500:  # å¢åŠ æœ€å¤§é•¿åº¦é™åˆ¶
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è¡¨æƒ…
        if self._is_pure_emoji(content):
            return False
        
        return True
    
    def _is_duplicate(self, barrage: Dict) -> bool:
        """å»é‡æ£€æµ‹"""
        content = barrage.get('content', '')
        user_id = barrage.get('user_id', '')
        
        # ç”Ÿæˆå»é‡key
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        timestamp = barrage.get('timestamp', time.time())
        
        # æ£€æŸ¥å†…å®¹å»é‡
        content_key = f"content_{content_hash}"
        if content_key in self.dedup_cache:
            last_time = self.dedup_cache.get(content_key)
            if timestamp - last_time < self.dedup_window:
                return True
        
        # æ£€æŸ¥ç”¨æˆ·å»é‡
        user_key = f"user_{user_id}_{content_hash}"
        if user_key in self.dedup_cache:
            return True
        
        # è®°å½•
        self.dedup_cache.put(content_key, timestamp)
        self.dedup_cache.put(user_key, timestamp)
        
        return False
    
    def _is_pure_emoji(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºçº¯è¡¨æƒ…"""
        # ç®€å•çš„è¡¨æƒ…æ£€æµ‹
        emoji_pattern = r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF]'
        text_without_emoji = re.sub(emoji_pattern, '', content)
        return len(text_without_emoji.strip()) == 0


class StreamingReplyProcessor:
    """æµå¼å›å¤å¤„ç†å™¨ï¼ˆç»“åˆAIå›å¤ç”Ÿæˆå™¨ï¼‰"""
    
    def __init__(self):
        self.reply_generator = create_reply_generator()
        self.current_sessions = {}
    
    async def generate_reply_stream(self, content: str) -> AsyncIterator[str]:
        """ç”Ÿæˆæµå¼å›å¤"""
        try:
            async for part in self.reply_generator.generate_reply_stream(content):
                yield part
                
        except Exception as e:
            logger.error(f"æµå¼å›å¤ç”Ÿæˆå¤±è´¥: {e}")
            yield "ç¨ç­‰å“¦ï¼Œå°åŠ©ç†é©¬ä¸Šå»é€šçŸ¥åº—å®¶ç¡®è®¤~"
    
    async def generate_reply(self, content: str) -> Optional[str]:
        """ç”Ÿæˆå®Œæ•´å›å¤"""
        try:
            result = await self.reply_generator.generate_reply(content)
            return result.text if result else None
            
        except Exception as e:
            logger.error(f"å›å¤ç”Ÿæˆå¤±è´¥: {e}")
            return "ç¨ç­‰å“¦ï¼Œå°åŠ©ç†é©¬ä¸Šå»é€šçŸ¥åº—å®¶ç¡®è®¤~"
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self.reply_generator.get_stats()
    
    async def close(self):
        """å…³é—­èµ„æº"""
        await self.reply_generator.close()


class OptimizedAIJudge:
    """ä¼˜åŒ–ç‰ˆAIåˆ¤æ–­å™¨ï¼ˆä½¿ç”¨æ–°çš„æ¶æ„ï¼‰"""
    
    def __init__(self):
        self.preprocessor = SmartBarrageProcessor()
        self.reply_processor = StreamingReplyProcessor()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_processed': 0,
            'ignored_count': 0,
            'ai_calls': 0,
            'cache_hits': 0,
            'batch_processed': 0,
            'template_replies': 0,
            'stream_replies': 0
        }
    
    async def process_barrage_stream(self, barrage: Dict) -> Optional[str]:
        """å¤„ç†å¼¹å¹•æµ"""
        self.stats['total_processed'] += 1
        
        # å¢åŠ è°ƒè¯•ä¿¡æ¯
        content = barrage.get('content', '')[:50]
        msg_type = barrage.get('type', 'unknown')
        logger.debug(f"ğŸ” å¤„ç†å¼¹å¹• [{msg_type}]: {content}...")
        
        # é¢„å¤„ç†
        result = await self.preprocessor.process_barrage(barrage)
        
        if result.action == ProcessAction.IGNORE:
            self.stats['ignored_count'] += 1
            logger.debug(f"âŒ å¿½ç•¥å¼¹å¹•: {result.reason}")
            return None
        
        elif result.action == ProcessAction.PROCESS:
            # ç«‹å³å¤„ç†
            logger.info(f"ğŸš€ ç«‹å³å¤„ç†å¼¹å¹• (ç½®ä¿¡åº¦: {result.confidence:.2f}): {content}")
            return await self._immediate_process(barrage)
        
        elif result.action == ProcessAction.BATCH:
            # æ‰¹å¤„ç†ï¼ˆæš‚æ—¶ç®€åŒ–ä¸ºç«‹å³å¤„ç†ï¼‰
            self.stats['batch_processed'] += 1
            logger.debug(f"ğŸ“¦ æ‰¹å¤„ç†å¼¹å¹• (ç½®ä¿¡åº¦: {result.confidence:.2f}): {content}")
            return await self._immediate_process(barrage)
        
        elif result.action == ProcessAction.DEFER:
            # å»¶è¿Ÿå¤„ç†
            logger.debug(f"â° å»¶è¿Ÿå¤„ç† {result.delay}ç§’: {content}")
            await asyncio.sleep(result.delay)
            return await self._immediate_process(barrage)
        
        return None
    
    async def _immediate_process(self, barrage: Dict) -> Optional[str]:
        """ç«‹å³å¤„ç†å¼¹å¹•"""
        try:
            content = barrage.get('content', '')
            
            # ç”Ÿæˆå›å¤
            self.stats['ai_calls'] += 1
            reply = await self.reply_processor.generate_reply(content)
            
            if reply:
                logger.info(f"âœ… ç”Ÿæˆå›å¤: {reply}")
                return reply
            
        except Exception as e:
            logger.error(f"ç«‹å³å¤„ç†å¤±è´¥: {e}")
        
        return None
    
    async def generate_reply_stream(self, content: str) -> AsyncIterator[str]:
        """ç”Ÿæˆæµå¼å›å¤"""
        self.stats['stream_replies'] += 1
        async for part in self.reply_processor.generate_reply_stream(content):
            yield part
    
    async def judge_barrages(self, barrages: List[Dict]) -> Optional[Dict]:
        """å…¼å®¹åŸæ¥å£çš„åˆ¤æ–­æ–¹æ³•"""
        if not barrages:
            return None
        
        # ä½¿ç”¨æ–°çš„å¤„ç†æµç¨‹
        for barrage in barrages:
            result = await self.process_barrage_stream(barrage)
            if result:
                category = self.preprocessor.keyword_filter.detect_category(barrage.get('content', ''))
                return {
                    'has_value': True,
                    'content': barrage.get('content', ''),
                    'reason': 'é€šè¿‡ä¼˜åŒ–æµç¨‹åˆ¤æ–­',
                    'category': category
                }
        
        return None
    
    async def generate_reply(self, content: str) -> Optional[str]:
        """å…¼å®¹åŸæ¥å£çš„å›å¤ç”Ÿæˆæ–¹æ³•"""
        return await self.reply_processor.generate_reply(content)
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        # åˆå¹¶æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
        reply_stats = self.reply_processor.get_stats()
        return {
            **self.stats,
            **reply_stats
        }
    
    async def close(self):
        """å…³é—­èµ„æº"""
        await self.reply_processor.close()


# å·¥å‚å‡½æ•°
def create_ai_judge():
    """åˆ›å»ºAIåˆ¤æ–­å™¨å®ä¾‹"""
    try:
        return OptimizedAIJudge()
    except Exception as e:
        logger.error(f"åˆ›å»ºAIåˆ¤æ–­å™¨å¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test():
        judge = create_ai_judge()
        if not judge:
            print("åˆ¤æ–­å™¨åˆ›å»ºå¤±è´¥")
            return
        
        test_barrages = [
            {"type": "chat", "content": "è¯·é—®ä½ ä»¬çš„èœå“ä»·æ ¼å¤šå°‘é’±", "user": "test1", "timestamp": time.time()},
            {"type": "chat", "content": "666", "user": "test2", "timestamp": time.time()},
            {"type": "chat", "content": "æˆ‘æƒ³é¢„çº¦ä½ç½®", "user": "test3", "timestamp": time.time()},
            {"type": "chat", "content": "ä½ ä»¬åœ°å€åœ¨å“ªé‡Œ", "user": "test4", "timestamp": time.time()},
        ]
        
        for barrage in test_barrages:
            print(f"\nå¼¹å¹•: {barrage['content']}")
            
            # æµ‹è¯•æ™®é€šå›å¤
            result = await judge.process_barrage_stream(barrage)
            print(f"å›å¤: {result}")
            
            # æµ‹è¯•æµå¼å›å¤
            if result:
                print("æµå¼: ", end="")
                async for part in judge.generate_reply_stream(barrage['content']):
                    print(part, end="", flush=True)
                print()
        
        print(f"\nç»Ÿè®¡ä¿¡æ¯: {judge.get_stats()}")
        await judge.close()
    
    asyncio.run(test()) 