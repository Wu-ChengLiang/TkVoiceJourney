 #!/usr/bin/env python3
"""
TkVoiceJourney é…ç½®æ–‡ä»¶
æ”¯æŒä¸åŒæ¨¡å‹å’ŒLoRAé…ç½®çš„åˆ‡æ¢
ä¸“ä¸ºRTX 4060 8GBæ˜¾å­˜ä¼˜åŒ–
"""

import os
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Dict, Any
import torch

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    name: str
    base_path: str
    lora_path: Optional[str] = None
    max_memory_gb: float = 7.0
    description: str = ""

@dataclass
class QuantizationConfig:
    """é‡åŒ–é…ç½®"""
    name: str
    load_in_4bit: bool = True
    load_in_8bit: bool = False
    bnb_4bit_quant_type: str = "nf4"
    bnb_4bit_compute_dtype: str = "float16"
    bnb_4bit_use_double_quant: bool = True
    description: str = ""

class TkVoiceConfig:
    """TkVoiceJourneyä¸»é…ç½®ç±»"""
    
    def __init__(self):
        self.current_dir = Path.cwd()
        self._setup_model_configs()
        self._setup_quantization_configs()
        
        # é»˜è®¤é…ç½®
        self.default_model = "qwen3_v10"
        self.default_quantization = "ultra_aggressive"
        
        # æ€§èƒ½é…ç½®
        self.max_input_length = 1024
        self.max_output_tokens = 200
        self.temperature = 0.2
        self.top_p = 0.9
        self.top_k = 40
        self.repetition_penalty = 1.05
        
        # TTSé…ç½®
        self.enable_tts = False
        self.tts_engine = "fish_speech"
        
    def _setup_model_configs(self):
        """è®¾ç½®æ¨¡å‹é…ç½®"""
        self.model_configs = {
            "qwen3_v7": ModelConfig(
                name="Qwen3 v7",
                base_path=str(self._find_base_model_path()),
                lora_path=str(self.current_dir.parent / "qwen3_output" / "v7-20250529-224742" / "checkpoint-33"),
                max_memory_gb=7.0,
                description="Qwen3-8B åŸºç¡€æ¨¡å‹ + v7 LoRAå¾®è°ƒ"
            ),
            "qwen3_v10": ModelConfig(
                name="Qwen3 v10",
                base_path=str(self._find_base_model_path()),
                lora_path=str(self.current_dir.parent / "qwen3_output" / "v10-20250530-074122" / "checkpoint-33"),
                max_memory_gb=7.0,
                description="Qwen3-8B åŸºç¡€æ¨¡å‹ + v10 LoRAå¾®è°ƒ"
            ),
            "qwen3_base": ModelConfig(
                name="Qwen3 Base",
                base_path=str(self._find_base_model_path()),
                lora_path=None,
                max_memory_gb=6.5,
                description="Qwen3-8B åŸºç¡€æ¨¡å‹ï¼ˆæ— LoRAï¼‰"
            )
        }
    
    def _setup_quantization_configs(self):
        """è®¾ç½®é‡åŒ–é…ç½®"""
        self.quantization_configs = {
            "ultra_aggressive": QuantizationConfig(
                name="Ultra Aggressive 4-bit",
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype="float16",
                bnb_4bit_use_double_quant=True,
                description="æœ€æ¿€è¿›4bité‡åŒ–ï¼Œæœ€å¤§åŒ–æ˜¾å­˜èŠ‚çœ"
            ),
            "balanced_4bit": QuantizationConfig(
                name="Balanced 4-bit",
                load_in_4bit=True,
                bnb_4bit_quant_type="fp4",
                bnb_4bit_compute_dtype="float16",
                bnb_4bit_use_double_quant=False,
                description="å¹³è¡¡çš„4bité‡åŒ–ï¼Œå…¼é¡¾æ€§èƒ½å’Œæ˜¾å­˜"
            ),
            "conservative_8bit": QuantizationConfig(
                name="Conservative 8-bit",
                load_in_4bit=False,
                load_in_8bit=True,
                bnb_4bit_compute_dtype="float16",
                description="ä¿å®ˆçš„8bité‡åŒ–ï¼Œä¼˜å…ˆä¿è¯ç²¾åº¦"
            ),
            "extreme_memory": QuantizationConfig(
                name="Extreme Memory Save",
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype="bfloat16",  # ä½¿ç”¨bfloat16è¿›ä¸€æ­¥èŠ‚çœ
                bnb_4bit_use_double_quant=True,
                description="æé™æ˜¾å­˜èŠ‚çœæ¨¡å¼ï¼ˆå¯èƒ½å½±å“ç²¾åº¦ï¼‰"
            )
        }
    
    def _find_base_model_path(self) -> Path:
        """æŸ¥æ‰¾åŸºç¡€æ¨¡å‹è·¯å¾„"""
        possible_paths = [
            self.current_dir / "my_models", 
            self.current_dir / "my_models_backup",
            self.current_dir.parent / "my_models",
            self.current_dir / "qwen3_models"
        ]
        
        for path in possible_paths:
            if path.exists() and (path / "config.json").exists():
                return path
                
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤è·¯å¾„
        return self.current_dir / "my_models"
    
    def get_model_config(self, model_name: str = None) -> ModelConfig:
        """è·å–æ¨¡å‹é…ç½®"""
        if model_name is None:
            model_name = self.default_model
        return self.model_configs.get(model_name, self.model_configs[self.default_model])
    
    def get_quantization_config(self, quant_name: str = None) -> QuantizationConfig:
        """è·å–é‡åŒ–é…ç½®"""
        if quant_name is None:
            quant_name = self.default_quantization
        return self.quantization_configs.get(quant_name, self.quantization_configs[self.default_quantization])
    
    def get_torch_dtype(self, compute_dtype: str) -> torch.dtype:
        """è·å–torchæ•°æ®ç±»å‹"""
        dtype_mapping = {
            "float16": torch.float16,
            "bfloat16": torch.bfloat16,
            "float32": torch.float32
        }
        return dtype_mapping.get(compute_dtype, torch.float16)
    
    def list_available_models(self) -> Dict[str, str]:
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        return {name: config.description for name, config in self.model_configs.items()}
    
    def list_available_quantizations(self) -> Dict[str, str]:
        """åˆ—å‡ºå¯ç”¨é‡åŒ–æ–¹å¼"""
        return {name: config.description for name, config in self.quantization_configs.items()}
    
    def validate_gpu_memory(self, required_gb: float = None) -> bool:
        """éªŒè¯GPUæ˜¾å­˜æ˜¯å¦è¶³å¤Ÿ"""
        if not torch.cuda.is_available():
            return False
            
        if required_gb is None:
            required_gb = self.get_model_config().max_memory_gb
            
        total_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        return total_memory_gb >= required_gb
    
    def get_memory_config(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä¼˜åŒ–é…ç½®"""
        return {
            "low_cpu_mem_usage": True,
            "device_map": "auto",
            # max_split_size_mb åº”è¯¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼Œä¸èƒ½ä¼ é€’ç»™æ¨¡å‹
        }
    
    def get_generation_config(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆé…ç½®"""
        return {
            "max_new_tokens": self.max_output_tokens,
            "do_sample": True,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "repetition_penalty": self.repetition_penalty,
            "use_cache": True,  # å¯ç”¨KVç¼“å­˜
        }

# å»¶è¿Ÿåˆ›å»ºå…¨å±€é…ç½®å®ä¾‹
def get_config():
    """è·å–é…ç½®å®ä¾‹"""
    if not hasattr(get_config, '_instance'):
        get_config._instance = TkVoiceConfig()
    return get_config._instance

# å‘åå…¼å®¹
config = None

def print_config_info():
    """æ‰“å°é…ç½®ä¿¡æ¯"""
    cfg = get_config()
    print("ğŸ”§ TkVoiceJourney é…ç½®ä¿¡æ¯")
    print("=" * 50)
    
    print("\nğŸ“¦ å¯ç”¨æ¨¡å‹:")
    for name, desc in cfg.list_available_models().items():
        marker = "âœ…" if name == cfg.default_model else "  "
        print(f"  {marker} {name}: {desc}")
    
    print("\nâš¡ é‡åŒ–æ–¹å¼:")
    for name, desc in cfg.list_available_quantizations().items():
        marker = "âœ…" if name == cfg.default_quantization else "  "
        print(f"  {marker} {name}: {desc}")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"\nğŸ”¥ GPU: {gpu_name}")
        print(f"ğŸ’¾ æ€»æ˜¾å­˜: {total_memory:.1f}GB")
        
        model_config = cfg.get_model_config()
        if cfg.validate_gpu_memory(model_config.max_memory_gb):
            print(f"âœ… æ˜¾å­˜è¶³å¤Ÿ (éœ€è¦ {model_config.max_memory_gb}GB)")
        else:
            print(f"âš ï¸  æ˜¾å­˜å¯èƒ½ä¸è¶³ (éœ€è¦ {model_config.max_memory_gb}GB)")
    else:
        print("\nâŒ æœªæ£€æµ‹åˆ°CUDA")

if __name__ == "__main__":
    print_config_info()