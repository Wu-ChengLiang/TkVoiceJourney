 #!/usr/bin/env python3
"""
TkVoiceJourney ä¼˜åŒ–å¯¹è¯è„šæœ¬
ä½¿ç”¨é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒæ¨¡å‹åˆ‡æ¢å’Œå¢å¼ºé‡åŒ–
åŒ…å«æ€ç»´é“¾è¿‡æ»¤åŠŸèƒ½
"""

import os
import sys
import torch
import gc
import re
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import warnings
warnings.filterwarnings("ignore")

# å¯¼å…¥é…ç½®
sys.path.append(str(Path(__file__).parent))
from config import get_config

class OptimizedChatModel:
    """ä¼˜åŒ–çš„å¯¹è¯æ¨¡å‹ç±»"""
    
    def __init__(self, model_name: str = None, quantization: str = None):
        self.config = get_config()
        self.model_name = model_name or self.config.default_model
        self.quantization = quantization or self.config.default_quantization
        self.model = None
        self.tokenizer = None
        self.conversation_history = []
        
    def clear_memory(self):
        """æ¸…ç†æ˜¾å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        print("ğŸš€ æ­£åœ¨åŠ è½½ä¼˜åŒ–æ¨¡å‹...")
        self.clear_memory()
        
        # è·å–é…ç½®
        model_config = self.config.get_model_config(self.model_name)
        quant_config = self.config.get_quantization_config(self.quantization)
        
        print(f"ğŸ“¦ æ¨¡å‹: {model_config.name}")
        print(f"âš¡ é‡åŒ–: {quant_config.name}")
        print(f"ğŸ“ åŸºç¡€æ¨¡å‹è·¯å¾„: {model_config.base_path}")
        
        # éªŒè¯æ˜¾å­˜
        if not self.config.validate_gpu_memory(model_config.max_memory_gb):
            print(f"âš ï¸  è­¦å‘Š: æ˜¾å­˜å¯èƒ½ä¸è¶³ï¼Œéœ€è¦ {model_config.max_memory_gb}GB")
        
        # è®¾ç½®é‡åŒ–é…ç½®
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=quant_config.load_in_4bit,
            load_in_8bit=quant_config.load_in_8bit,
            bnb_4bit_quant_type=quant_config.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=self.config.get_torch_dtype(quant_config.bnb_4bit_compute_dtype),
            bnb_4bit_use_double_quant=quant_config.bnb_4bit_use_double_quant,
        )
        
        try:
            # åŠ è½½tokenizer
            print("ğŸ“ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_config.base_path,
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            print("ğŸ§  åŠ è½½åŸºç¡€æ¨¡å‹...")
            memory_config = self.config.get_memory_config()
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_config.base_path,
                quantization_config=bnb_config,
                torch_dtype=self.config.get_torch_dtype(quant_config.bnb_4bit_compute_dtype),
                trust_remote_code=True,
                **memory_config
            )
            
            # åŠ è½½LoRAæ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if model_config.lora_path and Path(model_config.lora_path).exists():
                print("ğŸ”§ åŠ è½½LoRAé€‚é…å™¨...")
                self.model = PeftModel.from_pretrained(self.model, model_config.lora_path)
                print("âœ… LoRAæ¨¡å‹åŠ è½½æˆåŠŸ")
            else:
                print("âš ï¸  LoRAæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
            
            # è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
            self.model.eval()
            
            # æ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / (1024**3)
                print(f"ğŸ’¾ å½“å‰æ˜¾å­˜ä½¿ç”¨: {memory_used:.2f}GB")
            
            print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.clear_memory()
            return False
    
    def filter_thinking_chains(self, text: str) -> str:
        """è¿‡æ»¤æ€ç»´é“¾æ ‡è®°å’Œå†…å®¹"""
        # å»é™¤ <think>...</think> æ ‡è®°åŠå…¶å†…å®¹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # å»é™¤å…¶ä»–å¯èƒ½çš„æ€ç»´æ ‡è®°
        text = re.sub(r'<æ€è€ƒ>.*?</æ€è€ƒ>', '', text, flags=re.DOTALL)
        text = re.sub(r'\[æ€è€ƒ\].*?\[/æ€è€ƒ\]', '', text, flags=re.DOTALL)
        text = re.sub(r'\[æƒ³æ³•\].*?\[/æƒ³æ³•\]', '', text, flags=re.DOTALL)
        
        # æ¸…ç†å¤šä½™çš„ç©ºç™½å’Œæ¢è¡Œ
        text = re.sub(r'\n\s*\n', '\n', text)  # å»é™¤å¤šä½™ç©ºè¡Œ
        text = re.sub(r'^\s+|\s+$', '', text)  # å»é™¤é¦–å°¾ç©ºç™½
        
        return text.strip()
    
    def post_process_response(self, response: str) -> str:
        """åå¤„ç†å“åº”"""
        # è¿‡æ»¤æ€ç»´é“¾
        response = self.filter_thinking_chains(response)
        
        # å»é™¤assistant:æ ‡è®°
        response = response.split("assistant:")[0].strip()
        
        # å¤„ç†æ¢è¡Œé—®é¢˜ - åªä¿ç•™ç¬¬ä¸€æ®µæœ‰æ„ä¹‰çš„å›å¤
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        if lines:
            response = lines[0]
        
        # å¦‚æœå“åº”ä¸ºç©ºæˆ–åŒ…å«ä¸å½“å†…å®¹ï¼Œæä¾›é»˜è®¤å›å¤
        if not response or len(response) < 2:
            response = "ç¨ç­‰ï¼Œæˆ‘éœ€è¦æŸ¥è¯¢ä¸€ä¸‹"
        
        # ç¡®ä¿ä¸åŒ…å«ç”¨æˆ·è¾“å…¥çš„é‡å¤
        if 'user:' in response.lower():
            response = "ç¨ç­‰ä¸€ä¸‹ï¼Œæˆ‘å’Œè€å¸ˆç¡®è®¤ä¸€ä¸‹ï¼Œç¨åå›å¤æ‚¨"
        
        return response
    
    def format_conversation_prompt(self, user_input: str) -> str:
        """æ ¼å¼åŒ–å¯¹è¯prompt"""
        prompt_parts = []
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        prompt_parts.append("""ä½ æ˜¯ååŒ»å ‚çš„ç¾å›¢ä¸“ä¸šå®¢æœåŠ©ç†ï¼ŒååŒ»å ‚ç§‰æ‰¿'ä¼ æ‰¿ä¸­åŒ»æ–‡åŒ–ï¼ŒæœåŠ¡ç™¾å§“å¥åº·'ç†å¿µï¼Œä¸“æ³¨æä¾›'åŒ»å…»ç»“åˆ'çš„ä¸­åŒ»ç†ç–—æœåŠ¡ã€‚ååŒ»å ‚è®©ä¼˜è´¨ä¸­åŒ»èµ°è¿›ç¤¾åŒºï¼Œä¸ºé¡¾å®¢è§£å†³ç–¼ç—›ã€äºšå¥åº·åŠè„è…‘è°ƒç†é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. è¯­æ°”è¦ä¸“ä¸šã€ç®€çŸ­ã€æ¸©æŸ”ï¼Œä¸è¦ç”¨æ„Ÿå¹å·
2. ä¸è¦æ‰¿è¯ºä½ çŸ¥è¯†èŒƒç•´ä¹‹å¤–çš„äº‹æƒ…ï¼Œå¦‚æœè¶…å‡ºäº†ä½ çš„å›ç­”èŒƒå›´ï¼Œè¯·å›å¤ï¼šç¨ç­‰ä¸€ä¸‹ï¼Œæˆ‘å’Œè€å¸ˆç¡®è®¤ä¸€ä¸‹ï¼Œç¨åå›å¤æ‚¨
3. æ‰€æœ‰çš„å…·ä½“ä¿¡æ¯éƒ½ç”¨xxæ¥è¡¨ç¤ºï¼Œå¦‚ç”µè¯ã€å§“åã€æ—¶é—´ã€åœ°ç‚¹
4. ç›´æ¥å›ç­”ï¼Œä¸è¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹""")
        
        # æ·»åŠ å¯¹è¯å†å²ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
        for i in range(max(0, len(self.conversation_history) - 6), len(self.conversation_history), 2):
            if i + 1 < len(self.conversation_history):
                user_msg = self.conversation_history[i].replace("user: ", "")
                assistant_msg = self.conversation_history[i + 1].replace("assistant: ", "")
                prompt_parts.append(f"user: {user_msg}")
                prompt_parts.append(f"assistant: {assistant_msg}")
        
        # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥
        prompt_parts.append(f"user: {user_input}")
        prompt_parts.append("assistant: ")
        
        return "\n".join(prompt_parts)
    
    def generate_response(self, user_input: str) -> str:
        """ç”Ÿæˆå›å¤"""
        if self.model is None or self.tokenizer is None:
            return "æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåˆå§‹åŒ–æ¨¡å‹"
        
        try:
            # æ ¼å¼åŒ–prompt
            prompt = self.format_conversation_prompt(user_input)
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.config.max_input_length,
                truncation=True,
                padding=True
            ).to(self.model.device)
            
            # ç”Ÿæˆå›å¤
            generation_config = self.config.get_generation_config()
            generation_config.update({
                "pad_token_id": self.tokenizer.pad_token_id,
                "eos_token_id": self.tokenizer.eos_token_id,
            })
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    attention_mask=inputs.attention_mask,
                    **generation_config
                )
            
            # è§£ç å›å¤
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            # åå¤„ç†
            response = self.post_process_response(generated_text)
            
            # æ·»åŠ åˆ°å¯¹è¯å†å²
            self.conversation_history.append(f"user: {user_input}")
            self.conversation_history.append(f"assistant: {response}")
            
            # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†é•¿åº¦
            if len(self.conversation_history) > 12:
                self.conversation_history = self.conversation_history[-12:]
            
            # æ¸…ç†æ˜¾å­˜
            self.clear_memory()
            
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
            self.clear_memory()
            return "ç³»ç»Ÿå‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•"
    
    def chat_loop(self):
        """å¯¹è¯å¾ªç¯"""
        print("\nğŸ¯ ä¼˜åŒ–ç‰ˆç¾å›¢å®¢æœåŠ©æ‰‹å·²å°±ç»ªï¼")
        print("è¾“å…¥ 'quit'/'exit'/'é€€å‡º' ç»“æŸå¯¹è¯")
        print("è¾“å…¥ 'config' æŸ¥çœ‹å½“å‰é…ç½®")
        print("è¾“å…¥ 'memory' æŸ¥çœ‹æ˜¾å­˜ä½¿ç”¨æƒ…å†µ")
        print("=" * 50)
        
        # æµ‹è¯•é—®é¢˜å»ºè®®
        test_questions = [
            "åœ¨å—ï¼Ÿæƒ³é¢„çº¦ä¸‹åˆXXç‚¹",
            "æ–¹ä¾¿åœè½¦å—ï¼Ÿ",
            "XXè€å¸ˆåœ¨å—ï¼Ÿé¢„çº¦ä»–XXç‚¹",
            "ä½ ä»¬è¥ä¸šåˆ°å‡ ç‚¹ï¼Ÿ",
        ]

        print("ğŸ’¡ å»ºè®®æµ‹è¯•é—®é¢˜:")
        for i, q in enumerate(test_questions, 1):
            print(f"  {i}. {q}")
        
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nğŸª ç”¨æˆ·: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("ğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ç¾å›¢å®¢æœæœåŠ¡ï¼Œå†è§!")
                break
            elif user_input.lower() == 'config':
                from config import print_config_info
                print_config_info()
                continue
            elif user_input.lower() == 'memory':
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / (1024**3)
                    memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    print(f"ğŸ’¾ æ˜¾å­˜ä½¿ç”¨: {memory_used:.2f}GB / {memory_total:.1f}GB")
                else:
                    print("âŒ æœªæ£€æµ‹åˆ°CUDA")
                continue
            
            if not user_input:
                continue
            
            # ç”Ÿæˆå›å¤
            print("ğŸ¤– å®¢æœæ­£åœ¨ä¸ºæ‚¨æŸ¥è¯¢...")
            response = self.generate_response(user_input)
            print(f"ğŸ‘©â€ğŸ’¼ å®¢æœ: {response}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” TkVoiceJourney ä¼˜åŒ–ç‰ˆå®¢æœåŠ©æ‰‹")
    print("åŸºäºé…ç½®åŒ– Qwen3-8B + LoRA å¾®è°ƒæ¨¡å‹")
    print("ä¸“ä¸ºRTX 4060 8GBæ˜¾å­˜ä¼˜åŒ–")
    print("=" * 50)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
    
    # æ£€æŸ¥CUDA
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAï¼Œè¯·æ£€æŸ¥GPUé©±åŠ¨")
        return
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    from config import print_config_info
    print_config_info()
    
    # åˆ›å»ºèŠå¤©æ¨¡å‹
    chat_model = OptimizedChatModel()
    
    # åŠ è½½æ¨¡å‹
    if not chat_model.setup_model():
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # å¼€å§‹å¯¹è¯
    chat_model.chat_loop()
    
    # æ¸…ç†èµ„æº
    chat_model.clear_memory()
    print("ğŸ§¹ èµ„æºå·²æ¸…ç†")

if __name__ == "__main__":
    main()