#!/usr/bin/env python3
"""
ä½¿ç”¨æœ¬åœ°1.5Bæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒè„šæœ¬ - 4.5é˜¶æ®µä¼˜åŒ–ç‰ˆæœ¬
"""

import subprocess
import sys
import json
from pathlib import Path

def check_local_model():
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶"""
    model_path = Path("../my_models")
    model_file = model_path / "model.safetensors"
    
    print(f"ğŸ” æ£€æŸ¥æœ¬åœ°æ¨¡å‹: {model_path.absolute()}")
    
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
        return False
    
    if not model_file.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_file}")
        return False
    
    # æ£€æŸ¥å…¶ä»–å¿…è¦æ–‡ä»¶
    required_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json']
    missing_files = []
    
    for file in required_files:
        if not (model_path / file).exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"âš ï¸  ç¼ºå°‘æ–‡ä»¶: {missing_files}")
        print("è®­ç»ƒå¯èƒ½ä¼šé‡åˆ°é—®é¢˜ï¼Œä½†ä»ä¼šå°è¯•ç»§ç»­...")
    
    print(f"âœ… æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
    print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path.absolute()}")
    print(f"ğŸ“¦ æ¨¡å‹æ–‡ä»¶å¤§å°: {model_file.stat().st_size / (1024**3):.2f} GB")
    
    return True

def create_local_dataset():
    """åˆ›å»ºæœ¬åœ°è®­ç»ƒæ•°æ®é›†"""
    data_path = Path("./data/clean_train.json")
    
    if not data_path.exists():
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {data_path}")
        return None
    
    print(f"âœ… ä½¿ç”¨æ¸…æ´è®­ç»ƒæ•°æ®: {data_path}")
    return data_path

def start_local_training():
    """å¯åŠ¨æœ¬åœ°æ¨¡å‹è®­ç»ƒ"""
    
    print("ğŸš€ å¯åŠ¨4.5é˜¶æ®µï¼šé‡æ–°è®­ç»ƒä¼˜åŒ–åçš„æ¨¡å‹")
    print("ğŸ¯ é…ç½®: 4060ç¬”è®°æœ¬ 8GBæ˜¾å­˜")
    print("=" * 50)
    
    # æ£€æŸ¥æœ¬åœ°æ¨¡å‹
    if not check_local_model():
        print("âŒ æœ¬åœ°æ¨¡å‹æ£€æŸ¥å¤±è´¥")
        return 1
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®
    data_path = create_local_dataset()
    if not data_path:
        return 1
    
    # æœ¬åœ°æ¨¡å‹è·¯å¾„
    model_path = str(Path("../my_models").absolute())
    
    # è®­ç»ƒå‘½ä»¤ - 4.5é˜¶æ®µä¼˜åŒ–é…ç½®
    cmd = [
        'swift', 'sft',
        '--model', model_path,  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        '--model_type', 'qwen3',  # å‡è®¾æ˜¯qwen3æ¶æ„
        '--dataset', str(data_path.absolute()),
        '--template', 'chatml',
        '--output_dir', './output',
        '--num_train_epochs', '2',  # å‡å°‘åˆ°2è½®ï¼Œé¿å…è¿‡æ‹Ÿåˆ
        '--per_device_train_batch_size', '1',  # è¿›ä¸€æ­¥å‡å°‘batch sizeç¡®ä¿ç¨³å®š
        '--gradient_accumulation_steps', '8',  # è¡¥å¿batch_sizeå‡å°‘
        '--learning_rate', '1e-5',  # é™ä½å­¦ä¹ ç‡ï¼Œæ›´æ¸©å’Œçš„å­¦ä¹ 
        '--max_length', '256',  # å‡å°‘åˆ°256ï¼Œé¿å…å­¦ä¹ è¿‡é•¿åºåˆ—
        '--logging_steps', '1',
        '--save_steps', '25',  # æ›´é¢‘ç¹ä¿å­˜ï¼Œä¾¿äºæ—©åœ
        '--eval_steps', '25',  # æ›´é¢‘ç¹è¯„ä¼°
        '--lora_rank', '64',  # ä¿æŒè¾ƒå¤§çš„LoRA rank
        '--lora_alpha', '128',  # alpha = 2 * rank
        '--lora_dropout', '0.1',
        '--gradient_checkpointing',  # èŠ‚çœæ˜¾å­˜
        '--warmup_ratio', '0.05',  # å‡å°‘warmupæ¯”ä¾‹
        '--weight_decay', '0.01',
        '--lr_scheduler_type', 'cosine',
        '--save_total_limit', '3',  # åªä¿ç•™æœ€è¿‘3ä¸ªæ£€æŸ¥ç‚¹
        '--dataloader_num_workers', '2',  # å‡å°‘workeræ•°é‡
        '--fp16', 'true',  # ä½¿ç”¨FP16èŠ‚çœæ˜¾å­˜
        '--bf16', 'false',  # æ˜¾å¼ç¦ç”¨bf16
        '--torch_dtype', 'float16',  # è®¾ç½®torchæ•°æ®ç±»å‹
        '--report_to', 'tensorboard'  # å¯ç”¨tensorboardç›‘æ§
    ]
    
    print("ğŸ”§ è®­ç»ƒé…ç½®:")
    print(f"  ğŸ“¦ æ¨¡å‹: æœ¬åœ°1.5Bæ¨¡å‹ ({model_path})")
    with open(data_path, 'r', encoding='utf-8') as f:
        data_count = len(json.load(f))
    print(f"  ğŸ“Š æ•°æ®: {data_count}æ¡è®­ç»ƒæ ·æœ¬")
    print(f"  ğŸ¯ æ‰¹æ¬¡å¤§å°: 1 (æ¢¯åº¦ç´¯ç§¯: 8, æœ‰æ•ˆæ‰¹æ¬¡: 8)")
    print(f"  ğŸ“ åºåˆ—é•¿åº¦: 256")
    print(f"  ğŸ”„ è®­ç»ƒè½®æ•°: 2")
    print(f"  ğŸ“ˆ å­¦ä¹ ç‡: 1e-5")
    print(f"  ğŸ”§ LoRA: rank=64, alpha=128")
    print(f"  ğŸ’¾ æ˜¾å­˜ä¼˜åŒ–: FP16 + æ¢¯åº¦æ£€æŸ¥ç‚¹")
    print()
    
    print("â³ å¼€å§‹è®­ç»ƒ...")
    print("=" * 50)
    
    # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
    try:
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        
        # å®æ—¶æ˜¾ç¤ºè¾“å‡º
        step_count = 0
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                line = output.strip()
                print(line)
                
                # æ£€æµ‹è®­ç»ƒæ­¥æ•°
                if 'train_loss' in line or 'loss' in line:
                    step_count += 1
                    if step_count % 10 == 0:
                        print(f"ğŸ“Š å·²å®Œæˆ {step_count} æ­¥è®­ç»ƒ")
        
        # ç­‰å¾…è¿›ç¨‹å®Œæˆ
        return_code = process.wait()
        
        if return_code == 0:
            print("\nğŸ‰ 4.5é˜¶æ®µè®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print("ğŸ“ è¾“å‡ºç›®å½•:", Path("./output").absolute())
            print("ğŸ“Š TensorBoardæ—¥å¿—:", Path("./output").absolute() / "runs")
            print("\nğŸ’¡ æŸ¥çœ‹è®­ç»ƒæ•ˆæœ:")
            print("  tensorboard --logdir ./output/runs")
        else:
            print(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {return_code}")
            
        return return_code
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if process:
            process.terminate()
        return 1
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        return 1

if __name__ == "__main__":
    exit_code = start_local_training()
    sys.exit(exit_code) 