#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€LLMæµå¼å®¢æˆ·ç«¯
æ”¯æŒOpenAIå’ŒVLLMçš„æ— ç¼åˆ‡æ¢
"""

from openai import OpenAI
import asyncio
from typing import AsyncGenerator
import sys
import os

# æ·»åŠ ä¸Šçº§ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥config
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import (
    LLM_MODE, 
    OPENAI_BASE_URL, OPENAI_API_KEY, OPENAI_MODEL,
    VLLM_BASE_URL, VLLM_API_KEY, VLLM_MODEL,
    DEBUG
)


class UnifiedLLMClient:
    """
    ç»Ÿä¸€LLMæµå¼å®¢æˆ·ç«¯ (æ”¯æŒOpenAIå’ŒVLLMåˆ‡æ¢)
    """
    def __init__(self):
        """åˆå§‹åŒ–ç»Ÿä¸€LLMæµå¼å®¢æˆ·ç«¯"""
        self.mode = LLM_MODE
        
        if self.mode == "openai":
            # ä½¿ç”¨OpenAIé…ç½®
            self.client = OpenAI(
                base_url=OPENAI_BASE_URL,
                api_key=OPENAI_API_KEY,
            )
            self.model = OPENAI_MODEL
            if DEBUG:
                print(f"ğŸ”„ ä½¿ç”¨OpenAIæ¨¡å¼: {self.model}")
                print(f"ğŸŒ Base URL: {OPENAI_BASE_URL}")
                
        elif self.mode == "vllm":
            # ä½¿ç”¨VLLMé…ç½®
            if not VLLM_BASE_URL:
                raise ValueError("VLLM Base URL æœªé…ç½®ï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½® VLLM_BASE_URL")
                
            self.client = OpenAI(
                base_url=VLLM_BASE_URL,
                api_key=VLLM_API_KEY,
            )
            self.model = VLLM_MODEL
            if DEBUG:
                print(f"ğŸ”„ ä½¿ç”¨VLLMæ¨¡å¼: {self.model}")
                print(f"ğŸŒ Base URL: {VLLM_BASE_URL}")
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæ¨¡å¼: {self.mode}ï¼Œè¯·è®¾ç½®ä¸º 'openai' æˆ– 'vllm'")

    async def stream_chat(self, user_input: str, system_prompt: str = None) -> AsyncGenerator[str, None]:
        """
        æµå¼èŠå¤©ç”Ÿæˆ
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå¯é€‰
            
        Yields:
            str: æµå¼ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        # æ„å»ºæ¶ˆæ¯
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        else:
            # é»˜è®¤ç³»ç»Ÿæç¤ºè¯
            messages.append({
                "role": "system", 
                "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œç”¨ç®€æ´æ˜äº†çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"
            })
        
        messages.append({"role": "user", "content": user_input})

        try:
            # åˆ›å»ºæµå¼å“åº”
            stream = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7,
                stream=True,
                max_tokens=1000,
            )

            # æµå¼è¿”å›æ–‡æœ¬
            for chunk in stream:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    yield content
                    
        except Exception as e:
            error_msg = f"LLM API é”™è¯¯: {str(e)}"
            if DEBUG:
                print(f"âŒ {error_msg}")
            yield error_msg

    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "æµ‹è¯•è¿æ¥"}],
                max_tokens=10
            )
            print(f"âœ… {self.mode.upper()} è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ {self.mode.upper()} è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return False

    def get_models(self) -> list:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            models = self.client.models.list()
            return [model.id for model in models.data]
        except Exception as e:
            if DEBUG:
                print(f"âŒ è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def simple_chat(self, user_input: str, system_prompt: str = None) -> str:
        """
        ç®€å•èŠå¤©ï¼ˆéæµå¼ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼Œå¯é€‰
            
        Returns:
            str: å®Œæ•´å›å¤
        """
        response = ""
        async for chunk in self.stream_chat(user_input, system_prompt):
            response += chunk
        return response


# å‘åå…¼å®¹çš„åˆ«å
VLLMStreamClient = UnifiedLLMClient


async def main():
    """æµ‹è¯•ç”¨ä¾‹"""
    print(f"ğŸ§ª æµ‹è¯• {LLM_MODE.upper()} æµå¼å®¢æˆ·ç«¯...")
    
    try:
        client = UnifiedLLMClient()
        
        # æµ‹è¯•è¿æ¥
        if not client.test_connection():
            print("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return
        
        # æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨
        if DEBUG:
            models = client.get_models()
            if models:
                print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {models[:5]}...")  # æ˜¾ç¤ºå‰5ä¸ª
        
        # æµ‹è¯•æµå¼ç”Ÿæˆ
        print("\nğŸ”„ æµ‹è¯•æµå¼ç”Ÿæˆ:")
        print("åŠ©æ‰‹: ", end="", flush=True)
        
        async for chunk in client.stream_chat("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"):
            print(chunk, end='', flush=True)
        
        print("\n\nâœ… æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main()) 